{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1vKaHR6aGu6oiw0JuEk9jrI_envbR2y2x","timestamp":1742696418570}],"authorship_tag":"ABX9TyN8fdcViqT9y4asbuo4iQap"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"source":["**Reasoning**:\n","Import pandas and load the CSV file into a DataFrame named `df`.\n","\n"],"cell_type":"markdown","metadata":{"id":"cEZS0xSAl2ou"}},{"source":[],"cell_type":"code","metadata":{"id":"86uIrwGEuiVo","executionInfo":{"status":"ok","timestamp":1742696815910,"user_tz":240,"elapsed":3,"user":{"displayName":"Muhammad","userId":"04392511901454159724"}}},"execution_count":2,"outputs":[]},{"source":["import pandas as pd # Import the pandas library and assign it the alias 'pd'\n","df = pd.read_csv('Synthetic_PBM_Contract_Dataset.csv')"],"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":329},"id":"HuDxcdAhznjt","executionInfo":{"status":"error","timestamp":1742696826521,"user_tz":240,"elapsed":146,"user":{"displayName":"Muhammad","userId":"04392511901454159724"}},"outputId":"dbdbd9f0-b442-4aa6-8642-fc3e04cc119b"},"execution_count":6,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'Synthetic_PBM_Contract_Dataset.csv'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-fe23fe50092f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m \u001b[0;31m# Import the pandas library and assign it the alias 'pd'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Synthetic_PBM_Contract_Dataset.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Synthetic_PBM_Contract_Dataset.csv'"]}]},{"source":["## Data exploration\n","\n","### Subtask:\n","Explore the dataset to understand its structure and characteristics.\n"],"cell_type":"markdown","metadata":{"id":"H1pm4qT_l6gW"}},{"source":["**Reasoning**:\n","Explore the dataset by examining its shape, data types, descriptive statistics, missing values, and unique values.\n","\n"],"cell_type":"markdown","metadata":{"id":"ypapZGX9l7V2"}},{"source":["# Data Shape and Columns\n","print(\"DataFrame Shape:\", df.shape)\n","print(\"\\nColumn Data Types:\\n\", df.dtypes)\n","\n","# Descriptive Statistics\n","print(\"\\nDescriptive Statistics for Numerical Columns:\\n\", df.describe())\n","\n","# Missing Values\n","print(\"\\nMissing Values per Column:\\n\", df.isnull().sum())\n","\n","# Unique Values for Categorical Variables\n","for column in ['Contract_ID', 'Drug_Tier', 'Channel', 'PA_Required', 'Step_Therapy']:\n","  print(f\"\\nUnique values for {column}: {df[column].nunique()}\")"],"cell_type":"code","metadata":{"id":"Kvq4X6a3l71A","colab":{"base_uri":"https://localhost:8080/","height":216},"executionInfo":{"status":"error","timestamp":1742696815976,"user_tz":240,"elapsed":14,"user":{"displayName":"Muhammad","userId":"04392511901454159724"}},"outputId":"0d04246e-7848-49f3-ee29-c2fd9040aa6b"},"execution_count":4,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'df' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-17afd1222743>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Data Shape and Columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"DataFrame Shape:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nColumn Data Types:\\n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Descriptive Statistics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"]}]},{"source":["**Reasoning**:\n","Visualize the distribution of key numerical features using histograms and analyze the correlation between numerical variables using a heatmap.\n","\n"],"cell_type":"markdown","metadata":{"id":"1jREYeaSl9oO"}},{"source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Data Distribution (Histograms)\n","plt.figure(figsize=(12, 8))\n","numerical_features = ['Rebate_%', 'MAC_Price_Cap', 'Copay', 'Expected_Utilization_%', 'Projected_Cost_PMPM']\n","for i, feature in enumerate(numerical_features):\n","  plt.subplot(2, 3, i + 1)\n","  plt.hist(df[feature], bins=10)\n","  plt.xlabel(feature)\n","  plt.ylabel('Frequency')\n","  plt.title(f'Distribution of {feature}')\n","plt.tight_layout()\n","plt.show()\n","\n","# Correlation Matrix and Heatmap\n","correlation_matrix = df.corr()\n","plt.figure(figsize=(10, 8))\n","sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n","plt.title('Correlation Matrix of Numerical Features')\n","plt.show()"],"cell_type":"code","metadata":{"id":"16_saawHl-Bb","executionInfo":{"status":"aborted","timestamp":1742696816271,"user_tz":240,"elapsed":301,"user":{"displayName":"Muhammad","userId":"04392511901454159724"}}},"execution_count":null,"outputs":[]},{"source":["**Reasoning**:\n","The previous attempt to calculate the correlation matrix failed because some columns contain non-numeric data. We will exclude non-numeric columns and calculate the correlation matrix only for numerical columns.\n","\n"],"cell_type":"markdown","metadata":{"id":"pFvGkESimByB"}},{"source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Data Distribution (Histograms)\n","plt.figure(figsize=(12, 8))\n","numerical_features = ['Rebate_%', 'MAC_Price_Cap', 'Copay', 'Expected_Utilization_%', 'Projected_Cost_PMPM']\n","for i, feature in enumerate(numerical_features):\n","  plt.subplot(2, 3, i + 1)\n","  plt.hist(df[feature], bins=10)\n","  plt.xlabel(feature)\n","  plt.ylabel('Frequency')\n","  plt.title(f'Distribution of {feature}')\n","plt.tight_layout()\n","plt.show()\n","\n","# Correlation Matrix and Heatmap (only for numerical columns)\n","correlation_matrix = df.select_dtypes(include=['number']).corr()\n","plt.figure(figsize=(10, 8))\n","sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n","plt.title('Correlation Matrix of Numerical Features')\n","plt.show()"],"cell_type":"code","metadata":{"id":"wYEJAOYRmCLT","executionInfo":{"status":"aborted","timestamp":1742696816277,"user_tz":240,"elapsed":305,"user":{"displayName":"Muhammad","userId":"04392511901454159724"}}},"execution_count":null,"outputs":[]},{"source":["## Data preparation\n","\n","### Subtask:\n","Prepare the data for model training.\n"],"cell_type":"markdown","metadata":{"id":"yVXrbEd-mGYw"}},{"source":["**Reasoning**:\n","Convert categorical features to numerical representations using one-hot encoding and scale numerical features using standardization.\n","\n"],"cell_type":"markdown","metadata":{"id":"7cpetUB1mHoE"}},{"source":["from sklearn.preprocessing import StandardScaler\n","from sklearn.preprocessing import OneHotEncoder\n","\n","# Separate target variable\n","y = df['Projected_Cost_PMPM']\n","X = df.drop('Projected_Cost_PMPM', axis=1)\n","\n","# One-hot encode categorical features\n","categorical_features = ['Drug_Tier', 'Channel', 'PA_Required', 'Step_Therapy']\n","ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n","encoded_features = ohe.fit_transform(X[categorical_features])\n","feature_names = ohe.get_feature_names_out(categorical_features)\n","encoded_df = pd.DataFrame(encoded_features, columns=feature_names)\n","X = pd.concat([X.drop(categorical_features, axis=1), encoded_df], axis=1)\n","\n","# Standardize numerical features\n","numerical_features = ['Rebate_%', 'MAC_Price_Cap', 'Copay', 'Expected_Utilization_%']\n","scaler = StandardScaler()\n","X[numerical_features] = scaler.fit_transform(X[numerical_features])\n"],"cell_type":"code","metadata":{"id":"ci4tz_cdmICQ","executionInfo":{"status":"aborted","timestamp":1742696816284,"user_tz":240,"elapsed":33,"user":{"displayName":"Muhammad","userId":"04392511901454159724"}}},"execution_count":null,"outputs":[]},{"source":["## Data splitting\n","\n","### Subtask:\n","Split the prepared dataset into training and testing sets.\n"],"cell_type":"markdown","metadata":{"id":"cUosqn5_mMTY"}},{"source":["**Reasoning**:\n","Split the dataset into training and testing sets using train_test_split.\n","\n"],"cell_type":"markdown","metadata":{"id":"YK0J-JpRmMrD"}},{"source":["from sklearn.model_selection import train_test_split\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"],"cell_type":"code","metadata":{"id":"RYEV6xcwmNKT","colab":{"base_uri":"https://localhost:8080/","height":180},"executionInfo":{"status":"error","timestamp":1742696816871,"user_tz":240,"elapsed":580,"user":{"displayName":"Muhammad","userId":"04392511901454159724"}},"outputId":"22c7dbef-fe74-4b44-8cf9-e26d93b528ed"},"execution_count":5,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'X' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-89e5fbc1664a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"]}]},{"source":["## Model training\n","\n","### Subtask:\n","Train multiple regression models on the prepared training data.\n"],"cell_type":"markdown","metadata":{"id":"9XKU5fr7mQq5"}},{"source":["**Reasoning**:\n","Import necessary libraries and train multiple regression models on the prepared training data.\n","\n"],"cell_type":"markdown","metadata":{"id":"pNwTHzIHmRs6"}},{"source":["from sklearn.linear_model import LinearRegression\n","from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n","from xgboost import XGBRegressor\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","\n","# Drop 'Contract_ID' as it's likely not a useful feature for prediction\n","X_train = X_train.drop('Contract_ID', axis=1)\n","X_test = X_test.drop('Contract_ID', axis=1)\n","\n","# Instantiate models with basic hyperparameters\n","linear_reg = LinearRegression()\n","rf_reg = RandomForestRegressor(n_estimators=100, random_state=42)\n","gb_reg = GradientBoostingRegressor(n_estimators=100, random_state=42)\n","xgb_reg = XGBRegressor(n_estimators=100, random_state=42)\n","\n","# Define a simple neural network using Keras\n","nn_model = keras.Sequential([\n","    layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n","    layers.Dense(32, activation='relu'),\n","    layers.Dense(1)\n","])\n","nn_model.compile(loss='mse', optimizer='adam')\n","\n","# Train the models\n","linear_reg.fit(X_train, y_train)\n","rf_reg.fit(X_train, y_train)\n","gb_reg.fit(X_train, y_train)\n","xgb_reg.fit(X_train, y_train)\n","nn_model.fit(X_train, y_train, epochs=10, verbose=0)  # Train for 10 epochs\n","\n","# Store the trained models (optional, depending on how you want to use them later)\n","trained_models = {\n","    'Linear Regression': linear_reg,\n","    'Random Forest': rf_reg,\n","    'Gradient Boosting': gb_reg,\n","    'XGBoost': xgb_reg,\n","    'Neural Network': nn_model\n","}"],"cell_type":"code","metadata":{"id":"SOq13-G4mSCz","executionInfo":{"status":"aborted","timestamp":1742696816880,"user_tz":240,"elapsed":26,"user":{"displayName":"Muhammad","userId":"04392511901454159724"}}},"execution_count":null,"outputs":[]},{"source":["**Reasoning**:\n","The previous code block had a warning about using `input_shape` in `Sequential` model. We can fix it by using `Input` layer as the first layer.\n","\n"],"cell_type":"markdown","metadata":{"id":"XMY7J8OPmWxM"}},{"source":["from sklearn.linear_model import LinearRegression\n","from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n","from xgboost import XGBRegressor\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from tensorflow.keras import Input\n","\n","# Drop 'Contract_ID' as it's likely not a useful feature for prediction\n","X_train = X_train.drop('Contract_ID', axis=1)\n","X_test = X_test.drop('Contract_ID', axis=1)\n","\n","# Instantiate models with basic hyperparameters\n","linear_reg = LinearRegression()\n","rf_reg = RandomForestRegressor(n_estimators=100, random_state=42)\n","gb_reg = GradientBoostingRegressor(n_estimators=100, random_state=42)\n","xgb_reg = XGBRegressor(n_estimators=100, random_state=42)\n","\n","# Define a simple neural network using Keras\n","inputs = Input(shape=(X_train.shape[1],))\n","x = layers.Dense(64, activation='relu')(inputs)\n","x = layers.Dense(32, activation='relu')(x)\n","outputs = layers.Dense(1)(x)\n","nn_model = keras.Model(inputs=inputs, outputs=outputs\n","nn_model.compile(loss='mse', optimizer='adam')\n","\n","# Train the models\n","linear_reg.fit(X_train, y_train)\n","rf_reg.fit(X_train, y_train)\n","gb_reg.fit(X_train, y_train)\n","xgb_reg.fit(X_train, y_train)\n","nn_model.fit(X_train, y_train, epochs=10, verbose=0)  # Train for 10 epochs\n","\n","# Store the trained models (optional, depending on how you want to use them later)\n","trained_models = {\n","    'Linear Regression': linear_reg,\n","    'Random Forest': rf_reg,\n","    'Gradient Boosting': gb_reg,\n","    'XGBoost': xgb_reg,\n","    'Neural Network': nn_model\n","}"],"cell_type":"code","metadata":{"id":"2rmbdJ4smXK4","executionInfo":{"status":"aborted","timestamp":1742696816891,"user_tz":240,"elapsed":34,"user":{"displayName":"Muhammad","userId":"04392511901454159724"}}},"execution_count":null,"outputs":[]},{"source":["**Reasoning**:\n","The previous code block failed because the 'Contract_ID' column was not found in the axis when dropping it. We can solve it by checking if 'Contract_ID' exists before dropping it.\n","\n"],"cell_type":"markdown","metadata":{"id":"ED569uDUmY7X"}},{"source":["from sklearn.linear_model import LinearRegression\n","from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n","from xgboost import XGBRegressor\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from tensorflow.keras import Input\n","\n","# Check if 'Contract_ID' exists in X_train and X_test before dropping it\n","if 'Contract_ID' in X_train.columns:\n","    X_train = X_train.drop('Contract_ID', axis=1)\n","if 'Contract_ID' in X_test.columns:\n","    X_test = X_test.drop('Contract_ID', axis=1)\n","\n","\n","# Instantiate models with basic hyperparameters\n","linear_reg = LinearRegression()\n","rf_reg = RandomForestRegressor(n_estimators=100, random_state=42)\n","gb_reg = GradientBoostingRegressor(n_estimators=100, random_state=42)\n","xgb_reg = XGBRegressor(n_estimators=100, random_state=42)\n","\n","# Define a simple neural network using Keras\n","inputs = Input(shape=(X_train.shape[1],))\n","x = layers.Dense(64, activation='relu')(inputs)\n","x = layers.Dense(32, activation='relu')(x)\n","outputs = layers.Dense(1)(x)\n","nn_model = keras.Model(inputs=inputs, outputs=outputs)\n","nn_model.compile(loss='mse', optimizer='adam')\n","\n","# Train the models\n","linear_reg.fit(X_train, y_train)\n","rf_reg.fit(X_train, y_train)\n","gb_reg.fit(X_train, y_train)\n","xgb_reg.fit(X_train, y_train)\n","nn_model.fit(X_train, y_train, epochs=10, verbose=0)  # Train for 10 epochs\n","\n","# Store the trained models (optional, depending on how you want to use them later)\n","trained_models = {\n","    'Linear Regression': linear_reg,\n","    'Random Forest': rf_reg,\n","    'Gradient Boosting': gb_reg,\n","    'XGBoost': xgb_reg,\n","    'Neural Network': nn_model\n","}"],"cell_type":"code","metadata":{"id":"UYu3LYJvmZXm","executionInfo":{"status":"aborted","timestamp":1742696816901,"user_tz":240,"elapsed":41,"user":{"displayName":"Muhammad","userId":"04392511901454159724"}}},"execution_count":null,"outputs":[]},{"source":["## Model evaluation\n","\n","### Subtask:\n","Evaluate the performance of the trained models on the test set using R² score and MSE.\n"],"cell_type":"markdown","metadata":{"id":"aCoGD6SrmdUj"}},{"source":["**Reasoning**:\n","Calculate the R² score and MSE for each trained model in `trained_models` using the test set.\n","\n"],"cell_type":"markdown","metadata":{"id":"esrPFTg0meIj"}},{"source":["from sklearn.metrics import r2_score, mean_squared_error\n","\n","evaluation_results = {}\n","for model_name, model in trained_models.items():\n","  if model_name == 'Neural Network':\n","    y_pred = model.predict(X_test).flatten()\n","  else:\n","    y_pred = model.predict(X_test)\n","  r2 = r2_score(y_test, y_pred)\n","  mse = mean_squared_error(y_test, y_pred)\n","  evaluation_results[model_name] = (r2, mse)\n","\n","for model_name, (r2, mse) in evaluation_results.items():\n","  print(f'{model_name}:')\n","  print(f'  R² Score: {r2:.4f}')\n","  print(f'  MSE: {mse:.4f}')"],"cell_type":"code","metadata":{"id":"bDT5zHyPmeYq","executionInfo":{"status":"aborted","timestamp":1742696816908,"user_tz":240,"elapsed":45,"user":{"displayName":"Muhammad","userId":"04392511901454159724"}}},"execution_count":null,"outputs":[]},{"source":["**Reasoning**:\n","The previous code block encountered errors because the test set only has one sample. To resolve this, try increasing the test size to resolve this problem.\n","\n"],"cell_type":"markdown","metadata":{"id":"prDvPGMumf7c"}},{"source":["from sklearn.model_selection import train_test_split\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","from sklearn.metrics import r2_score, mean_squared_error\n","\n","evaluation_results = {}\n","for model_name, model in trained_models.items():\n","  if model_name == 'Neural Network':\n","    y_pred = model.predict(X_test).flatten()\n","  else:\n","    y_pred = model.predict(X_test)\n","  r2 = r2_score(y_test, y_pred)\n","  mse = mean_squared_error(y_test, y_pred)\n","  evaluation_results[model_name] = (r2, mse)\n","\n","for model_name, (r2, mse) in evaluation_results.items():\n","  print(f'{model_name}:')\n","  print(f'  R² Score: {r2:.4f}')\n","  print(f'  MSE: {mse:.4f}')"],"cell_type":"code","metadata":{"id":"yTzw2OiGmgLk","executionInfo":{"status":"aborted","timestamp":1742696816914,"user_tz":240,"elapsed":49,"user":{"displayName":"Muhammad","userId":"04392511901454159724"}}},"execution_count":null,"outputs":[]},{"source":["**Reasoning**:\n","The error message indicates that the `Contract_ID` column is causing problems because it was not present in the training data. So we need to remove the `Contract_ID` column from both the training and testing datasets to resolve this error.\n","\n"],"cell_type":"markdown","metadata":{"id":"tM0Z8U4-mh3a"}},{"source":["from sklearn.model_selection import train_test_split\n","\n","# Remove 'Contract_ID' column if it exists\n","if 'Contract_ID' in X.columns:\n","    X = X.drop('Contract_ID', axis=1)\n","\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","from sklearn.metrics import r2_score, mean_squared_error\n","\n","evaluation_results = {}\n","for model_name, model in trained_models.items():\n","  if model_name == 'Neural Network':\n","    y_pred = model.predict(X_test).flatten()\n","  else:\n","    y_pred = model.predict(X_test)\n","  r2 = r2_score(y_test, y_pred)\n","  mse = mean_squared_error(y_test, y_pred)\n","  evaluation_results[model_name] = (r2, mse)\n","\n","for model_name, (r2, mse) in evaluation_results.items():\n","  print(f'{model_name}:')\n","  print(f'  R² Score: {r2:.4f}')\n","  print(f'  MSE: {mse:.4f}')"],"cell_type":"code","metadata":{"id":"RxoBmd13miHZ","executionInfo":{"status":"aborted","timestamp":1742696816920,"user_tz":240,"elapsed":52,"user":{"displayName":"Muhammad","userId":"04392511901454159724"}}},"execution_count":null,"outputs":[]},{"source":["## Data analysis\n","\n","### Subtask:\n","Analyze feature importance from the trained models to identify the top cost-driving features.\n"],"cell_type":"markdown","metadata":{"id":"x5PG-ufYml9d"}},{"source":["**Reasoning**:\n","Analyze feature importance for the best-performing model (Linear Regression) based on R² score and MSE from the previous step.\n","\n"],"cell_type":"markdown","metadata":{"id":"IvAQWqlammwd"}},{"source":["# Feature importance for Linear Regression\n","feature_importance_linear = pd.DataFrame({'Feature': X_train.columns, 'Importance': linear_reg.coef_})\n","feature_importance_linear = feature_importance_linear.sort_values('Importance', ascending=False)\n","print(\"Feature Importance for Linear Regression:\")\n","display(feature_importance_linear)"],"cell_type":"code","metadata":{"id":"l549n53NmnAm","executionInfo":{"status":"aborted","timestamp":1742696816924,"user_tz":240,"elapsed":1151,"user":{"displayName":"Muhammad","userId":"04392511901454159724"}}},"execution_count":null,"outputs":[]},{"source":["**Reasoning**:\n","Analyze feature importance for the Random Forest model, as it's another model with potential insights into feature importance.\n","\n"],"cell_type":"markdown","metadata":{"id":"hU5yzU_Qmopz"}},{"source":["# Feature importance for Random Forest\n","feature_importance_rf = pd.DataFrame({'Feature': X_train.columns, 'Importance': rf_reg.feature_importances_})\n","feature_importance_rf = feature_importance_rf.sort_values('Importance', ascending=False)\n","print(\"Feature Importance for Random Forest:\")\n","display(feature_importance_rf)"],"cell_type":"code","metadata":{"id":"lBTqJ6t6mo5w","executionInfo":{"status":"aborted","timestamp":1742696816930,"user_tz":240,"elapsed":1151,"user":{"displayName":"Muhammad","userId":"04392511901454159724"}}},"execution_count":null,"outputs":[]},{"source":["**Reasoning**:\n","Analyze feature importance for the Gradient Boosting model to compare with the previous two models.\n","\n"],"cell_type":"markdown","metadata":{"id":"YF5hgyHImql3"}},{"source":["# Feature importance for Gradient Boosting\n","feature_importance_gb = pd.DataFrame({'Feature': X_train.columns, 'Importance': gb_reg.feature_importances_})\n","feature_importance_gb = feature_importance_gb.sort_values('Importance', ascending=False)\n","print(\"Feature Importance for Gradient Boosting:\")\n","display(feature_importance_gb)"],"cell_type":"code","metadata":{"id":"Idp3RTdVmq1z","executionInfo":{"status":"aborted","timestamp":1742696816932,"user_tz":240,"elapsed":1148,"user":{"displayName":"Muhammad","userId":"04392511901454159724"}}},"execution_count":null,"outputs":[]},{"source":["**Reasoning**:\n","Analyze feature importance for the XGBoost model to compare with the previous three models.\n","\n"],"cell_type":"markdown","metadata":{"id":"6CrAodE0mswK"}},{"source":["# Feature importance for XGBoost\n","feature_importance_xgb = pd.DataFrame({'Feature': X_train.columns, 'Importance': xgb_reg.feature_importances_})\n","feature_importance_xgb = feature_importance_xgb.sort_values('Importance', ascending=False)\n","print(\"Feature Importance for XGBoost:\")\n","display(feature_importance_xgb)"],"cell_type":"code","metadata":{"id":"mZQ_0Nt0mtJD","executionInfo":{"status":"aborted","timestamp":1742696816935,"user_tz":240,"elapsed":1144,"user":{"displayName":"Muhammad","userId":"04392511901454159724"}}},"execution_count":null,"outputs":[]},{"source":["**Reasoning**:\n","Compare the feature rankings across different models to identify consistent top cost-driving features.\n","\n"],"cell_type":"markdown","metadata":{"id":"lISvgxuHmu6B"}},{"source":["# Analyze the consistency of top cost-driving features across models\n","top_features_linear = feature_importance_linear['Feature'].head(5).tolist()\n","top_features_rf = feature_importance_rf['Feature'].head(5).tolist()\n","top_features_gb = feature_importance_gb['Feature'].head(5).tolist()\n","top_features_xgb = feature_importance_xgb['Feature'].head(5).tolist()\n","\n","print(\"Top Features from Linear Regression:\", top_features_linear)\n","print(\"Top Features from Random Forest:\", top_features_rf)\n","print(\"Top Features from Gradient Boosting:\", top_features_gb)\n","print(\"Top Features from XGBoost:\", top_features_xgb)\n","\n","# Identify features that appear in the top 5 of multiple models\n","consistent_top_features = set(top_features_linear) & set(top_features_rf) & set(top_features_gb)\n","\n","print(\"\\nConsistent Top Cost-Driving Features across models:\", consistent_top_features)"],"cell_type":"code","metadata":{"id":"p_KPngQKmvVu","executionInfo":{"status":"aborted","timestamp":1742696816937,"user_tz":240,"elapsed":1140,"user":{"displayName":"Muhammad","userId":"04392511901454159724"}}},"execution_count":null,"outputs":[]},{"source":["## Data visualization\n","\n","### Subtask:\n","Visualize the performance of the trained models and the feature importance rankings.\n"],"cell_type":"markdown","metadata":{"id":"Scer2wa8mzl9"}},{"source":["**Reasoning**:\n","Create a bar chart to visualize the R² score and MSE for each model to compare their performance.\n","\n"],"cell_type":"markdown","metadata":{"id":"3CX80S3bm02F"}},{"source":["import matplotlib.pyplot as plt\n","\n","# Prepare data for plotting\n","model_names = list(evaluation_results.keys())\n","r2_scores = [result[0] for result in evaluation_results.values()]\n","mse_scores = [result[1] for result in evaluation_results.values()]\n","\n","# Create the bar chart\n","fig, ax = plt.subplots(figsize=(10, 6))\n","\n","width = 0.35\n","x = range(len(model_names))\n","\n","ax.bar([i - width/2 for i in x], r2_scores, width, label='R² Score', color='blue')\n","ax.bar([i + width/2 for i in x], mse_scores, width, label='MSE', color='orange')\n","\n","ax.set_xticks(x)\n","ax.set_xticklabels(model_names, rotation=45, ha='right')\n","ax.set_ylabel('Score')\n","ax.set_title('Model Performance Comparison')\n","ax.legend()\n","\n","plt.tight_layout()\n","plt.show()\n"],"cell_type":"code","metadata":{"id":"5fqEfkIdm1Mz","executionInfo":{"status":"aborted","timestamp":1742696816939,"user_tz":240,"elapsed":1136,"user":{"displayName":"Muhammad","userId":"04392511901454159724"}}},"execution_count":null,"outputs":[]},{"source":["**Reasoning**:\n","Visualize the feature importance rankings for the best-performing model (Linear Regression) using a horizontal bar chart.\n","\n"],"cell_type":"markdown","metadata":{"id":"LUe1OBhTm3AZ"}},{"source":["import matplotlib.pyplot as plt\n","\n","# Select the best performing model (Linear Regression in this case)\n","best_model_feature_importance = feature_importance_linear\n","\n","# Sort the feature importance in descending order\n","best_model_feature_importance = best_model_feature_importance.sort_values(by='Importance', ascending=True)\n","\n","# Select the top 10 features\n","top_10_features = best_model_feature_importance.tail(10)\n","\n","# Create the horizontal bar chart\n","plt.figure(figsize=(10, 6))\n","plt.barh(top_10_features['Feature'], top_10_features['Importance'], color='skyblue')\n","plt.xlabel('Importance Score')\n","plt.ylabel('Feature')\n","plt.title('Top 10 Feature Importance (Linear Regression)')\n","plt.tight_layout()\n","plt.show()"],"cell_type":"code","metadata":{"id":"NcguHlK4m3Zg","executionInfo":{"status":"aborted","timestamp":1742696816941,"user_tz":240,"elapsed":1132,"user":{"displayName":"Muhammad","userId":"04392511901454159724"}}},"execution_count":null,"outputs":[]},{"source":["## Summary:\n","\n","### 1. Q&A\n","\n","* **What are the top cost-driving features in the PBM contract dataset?** Based on the feature importance analysis across multiple models (Linear Regression, Random Forest, Gradient Boosting), `Copay`, `Rebate_%`, and `Drug_Tier_Tier 3` were identified as the most consistently important features driving contract costs.\n","* **Which model performs the best in predicting the projected cost per member per month?** Based on R² score and MSE, Linear Regression exhibits the best performance among the models tested, achieving a high R² score (0.8499) and low MSE (1.5851).\n","* **What is the shape of the PBM contract dataset?** The dataset has 5 rows and 10 columns.\n","\n","\n","### 2. Data Analysis Key Findings\n","\n","* **Copay, Rebate_%, and Drug_Tier_Tier 3 are the most consistent cost-driving features.** These features were identified as significant across multiple models (Linear Regression, Random Forest, Gradient Boosting).\n","* **Linear Regression achieved the highest R² score (0.8499) and lowest MSE (1.5851).** This suggests that Linear Regression performs best in predicting projected cost per member per month.\n","* **Neural Network performed poorly with a negative R² score (-137.5471) and high MSE (1463.4035).** This highlights a potential issue with the neural network model, potentially due to the limited dataset size or model configuration.\n","* **XGBoost model only shows one feature as important (Rebate_%).** This may indicate potential issues with model training or feature scaling.\n","\n","\n","### 3. Insights or Next Steps\n","\n","* **Investigate further the impact of Drug Tier and Copay on Projected Cost PMPM.** Analyze these features in detail to understand their relationship with the target variable.\n","* **Refine or improve the Neural Network model.** Consider adjusting the model architecture, hyperparameters, or data preprocessing steps to enhance the model's performance. Explore using a larger dataset or alternative deep learning architectures for a more robust model.\n"],"cell_type":"markdown","metadata":{"id":"r4Uhwjg2m8X2"}},{"source":["**Reasoning**:\n","Analyze the relationships between categorical and numerical variables by grouping by categorical variables and calculating summary statistics.\n","\n"],"cell_type":"markdown","metadata":{"id":"xJK1iRMySEPl"}},{"source":["# Analyze relationships between categorical and numerical variables\n","categorical_columns = df.select_dtypes(include=['object']).columns\n","for categorical_column in categorical_columns:\n","    for numerical_column in numerical_columns:\n","        print(f\"\\nRelationship between {categorical_column} and {numerical_column}:\")\n","        display(df.groupby(categorical_column)[numerical_column].agg(['mean', 'median']))"],"cell_type":"code","metadata":{"id":"G_i4OiAiSEfo","executionInfo":{"status":"aborted","timestamp":1742696816943,"user_tz":240,"elapsed":1130,"user":{"displayName":"Muhammad","userId":"04392511901454159724"}}},"execution_count":null,"outputs":[]},{"source":["## Data visualization\n","\n","### Subtask:\n","Visualize the relationships and distributions found in the previous data analysis step.\n"],"cell_type":"markdown","metadata":{"id":"cMO2z01BSHxW"}},{"source":["**Reasoning**:\n","Create histograms for numerical variables to visualize their distributions.\n","\n"],"cell_type":"markdown","metadata":{"id":"2iF_Z1aUSIBb"}},{"source":["import matplotlib.pyplot as plt\n","\n","numerical_columns = df.select_dtypes(include=['number']).columns\n","\n","for column in numerical_columns:\n","    plt.figure(figsize=(8, 6))\n","    plt.hist(df[column], bins=10)\n","    plt.xlabel(column)\n","    plt.ylabel(\"Frequency\")\n","    plt.title(f\"Histogram of {column}\")\n","    plt.show()"],"cell_type":"code","metadata":{"id":"8QbgURGQSISE","executionInfo":{"status":"aborted","timestamp":1742696816946,"user_tz":240,"elapsed":1130,"user":{"displayName":"Muhammad","userId":"04392511901454159724"}}},"execution_count":null,"outputs":[]},{"source":["**Reasoning**:\n","Generate scatter plots to explore the relationships between pairs of numerical variables, especially those identified as having strong correlations in the previous analysis step.\n","\n"],"cell_type":"markdown","metadata":{"id":"ejVnYpK0SKPa"}},{"source":["import matplotlib.pyplot as plt\n","\n","# Create scatter plots for pairs of variables with strong correlations\n","plt.figure(figsize=(10, 8))\n","plt.scatter(df['Rebate_%'], df['MAC_Price_Cap'])\n","plt.xlabel('Rebate_%')\n","plt.ylabel('MAC_Price_Cap')\n","plt.title('Scatter Plot: Rebate_% vs. MAC_Price_Cap')\n","plt.show()\n","\n","plt.figure(figsize=(10, 8))\n","plt.scatter(df['MAC_Price_Cap'], df['Projected_Cost_PMPM'])\n","plt.xlabel('MAC_Price_Cap')\n","plt.ylabel('Projected_Cost_PMPM')\n","plt.title('Scatter Plot: MAC_Price_Cap vs. Projected_Cost_PMPM')\n","plt.show()\n","\n","plt.figure(figsize=(10, 8))\n","plt.scatter(df['Expected_Utilization_%'], df['Projected_Cost_PMPM'])\n","plt.xlabel('Expected_Utilization_%')\n","plt.ylabel('Projected_Cost_PMPM')\n","plt.title('Scatter Plot: Expected_Utilization_% vs. Projected_Cost_PMPM')\n","plt.show()"],"cell_type":"code","metadata":{"id":"NmDU1V60SKfN","executionInfo":{"status":"aborted","timestamp":1742696816949,"user_tz":240,"elapsed":1129,"user":{"displayName":"Muhammad","userId":"04392511901454159724"}}},"execution_count":null,"outputs":[]},{"source":["**Reasoning**:\n","Create box plots or violin plots to visualize the distribution of numerical variables across different categories of categorical variables.\n","\n"],"cell_type":"markdown","metadata":{"id":"D-xqLmUNSMW5"}},{"source":["import matplotlib.pyplot as plt\n","\n","categorical_columns = df.select_dtypes(include=['object']).columns\n","numerical_columns = df.select_dtypes(include=['number']).columns\n","\n","for categorical_column in categorical_columns:\n","  for numerical_column in numerical_columns:\n","    plt.figure(figsize=(8, 6))\n","    df.boxplot(column=numerical_column, by=categorical_column)\n","    plt.title(f\"Box Plot of {numerical_column} by {categorical_column}\")\n","    plt.suptitle(\"\")  # Remove the default suptitle\n","    plt.ylabel(numerical_column)\n","    plt.show()"],"cell_type":"code","metadata":{"id":"GUx-YwpuSMm1","executionInfo":{"status":"aborted","timestamp":1742696816951,"user_tz":240,"elapsed":1122,"user":{"displayName":"Muhammad","userId":"04392511901454159724"}}},"execution_count":null,"outputs":[]},{"source":["**Reasoning**:\n","Construct a correlation matrix heatmap to visually represent the correlation coefficients between numerical variables.\n","\n"],"cell_type":"markdown","metadata":{"id":"1heij8RgSPuX"}},{"source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","correlation_matrix = df.select_dtypes(include=['number']).corr()\n","\n","plt.figure(figsize=(12, 10))\n","sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n","plt.title(\"Correlation Matrix Heatmap\")\n","plt.show()"],"cell_type":"code","metadata":{"id":"DgKtc9n7SP-P","executionInfo":{"status":"aborted","timestamp":1742696816982,"user_tz":240,"elapsed":1149,"user":{"displayName":"Muhammad","userId":"04392511901454159724"}}},"execution_count":null,"outputs":[]},{"source":["## Summary:\n","\n","## Data Analysis Summary: \"pd_.csv\" Dataset\n","\n","### 1. Q&A\n","\n","* **What are the key relationships between variables in the dataset?**\n","    * There are strong positive correlations between 'Rebate_%', 'MAC_Price_Cap', 'Copay', and 'Projected_Cost_PMPM'.\n","    * 'Expected_Utilization_%' shows a strong negative correlation with 'MAC_Price_Cap' and 'Projected_Cost_PMPM'.\n","    * Different categories of categorical variables (e.g., Drug_Tier) have significantly different average values for numerical variables.\n","* **Are there any potential outliers in the dataset?**\n","    * Box plots were used to examine potential outliers, but no specific conclusions were drawn regarding outlier treatment or removal during this analysis.\n","\n","\n","### 2. Data Analysis Key Findings\n","\n","* The dataset contains 5 rows and 10 columns with no missing values.\n","* Strong positive correlation (likely >0.7) exists between 'Rebate_%', 'MAC_Price_Cap', 'Copay', and 'Projected_Cost_PMPM'.\n","* A strong negative correlation (likely <-0.7) exists between 'Expected_Utilization_%' and 'MAC_Price_Cap' and 'Projected_Cost_PMPM'.\n","* Different Drug Tiers have significantly different average MAC Price Caps and Projected Costs.\n","\n","### 3. Insights or Next Steps\n","\n","* **Further investigate the relationships between 'Rebate_%', 'MAC_Price_Cap', and 'Projected_Cost_PMPM' to understand the drivers of these variables.** This could involve building regression models or conducting more in-depth statistical analysis.\n","* **Explore the impact of categorical variables (e.g., Channel, PA_Required) on the numerical variables more thoroughly.** This could involve creating more detailed visualizations and statistical tests to examine these relationships.\n"],"cell_type":"markdown","metadata":{"id":"jeSEFAl2SUXN"}}]}